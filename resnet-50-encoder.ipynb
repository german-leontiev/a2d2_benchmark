{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e7ea8a2-0d10-4b8b-a501-0be37ef8d2d5",
   "metadata": {},
   "source": [
    "### Import libs, global variables and pickled files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba04c81c-14ce-48f8-9385-b1de128cf4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os, cv2, random, tqdm, json\n",
    "from random import choice as r_ch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from os.path import join as join_path\n",
    "from os.path import sep as os_sep\n",
    "from os.path import exists as path_exists\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as album\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from global_names import A2D2_PATH\n",
    "\n",
    "\n",
    "with open(\"bm_ds.pkl\", \"rb\") as f:\n",
    "    bm_ds = pickle.load(f)\n",
    "    \n",
    "# Load files\n",
    "with open(join_path(A2D2_PATH, \"camera_lidar_semantic\", \"class_list.json\"), \"rb\") as f:\n",
    "     class_list= json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9a7a8c-1d12-4e88-a431-471a9a3716f3",
   "metadata": {},
   "source": [
    "### Define functions for getting paths from ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d32b6aad-6fb9-47c0-ae25-d5bc662cd26d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ss_p = join_path(A2D2_PATH, \"camera_lidar_semantic\")\n",
    "\n",
    "sens_ext = {\n",
    "    \"camera\": \".png\",\n",
    "    \"label\": \".png\",\n",
    "    \"lidar\": \".npz\"\n",
    "}\n",
    "\n",
    "rel_ = lambda __p: join_path(*__p.split(os_sep)[__p.split(os_sep).index('camera_lidar_semantic'):])\n",
    "abs_ = lambda __p: join_path(A2D2_PATH, __p)\n",
    "\n",
    "def sensor_p(_id, s_type):\n",
    "    if s_type not in sens_ext.keys(): raise ValueError(\"Wrong sensor type: s_type\")\n",
    "    d,t,s = _id.split(\"_\")\n",
    "    _p = \"_\".join([d, s_type, s, t]) + sens_ext[s_type]\n",
    "    _p = join_path(ss_p, f\"{d[:8]}_{d[8:]}\", s_type, f\"cam_{sa_(s)}\", _p)\n",
    "    return rel_(_p)\n",
    "\n",
    "def sa_(x):\n",
    "    als = [\"center\", \"left\", \"right\"]\n",
    "    for o in als:\n",
    "        if o in x:\n",
    "            return x.replace(o, \"_\" + o)\n",
    "    raise ValueError(f\"Bad index contains wrong sensor align: {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8b5f8-92ae-47e4-be33-6d7bf775369f",
   "metadata": {},
   "source": [
    "### Create dataset from sparced ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b824e7b5-cc3c-4fbc-8489-53b208175bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's important to reduce number of images\n",
    "sparced_ids = [val for i, val in enumerate(bm_ds[\"train_ids\"]) if i % 10 == 0]\n",
    "x_train_dir = np.array([abs_(sensor_p(p, \"camera\")) for p in sparced_ids])\n",
    "y_train_dir = np.array([abs_(sensor_p(p, \"label\")) for p in sparced_ids])\n",
    "\n",
    "# It's important to reduce number of images\n",
    "sparced_val_ids = [val for i, val in enumerate(bm_ds[\"val_ids\"]) if i % 10 == 0]\n",
    "val_images_paths = np.array([abs_(sensor_p(p, \"camera\")) for p in sparced_val_ids])\n",
    "val_labels_paths = np.array([abs_(sensor_p(p, \"label\")) for p in sparced_val_ids])\n",
    "\n",
    "class_names = list(class_list.values())\n",
    "class_rgb_values = [[int(i[1:3], 16), int(i[3:5], 16), int(i[5:7], 16)] for i in class_list.keys()]\n",
    "\n",
    "# Useful to shortlist specific classes in datasets with large number of classes\n",
    "select_classes = class_names # all classes\n",
    "\n",
    "# Get RGB values of required classes\n",
    "select_class_indices = [class_names.index(cls) for cls in select_classes]\n",
    "select_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n",
    "\n",
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"\n",
    "    Plot images in one row\n",
    "    \"\"\"\n",
    "    n_images = len(images)\n",
    "    plt.figure(figsize=(20,8))\n",
    "    for idx, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n_images, idx + 1)\n",
    "        plt.xticks([]); \n",
    "        plt.yticks([])\n",
    "        # get title from the parameter names\n",
    "        plt.title(name.replace('_',' ').title(), fontsize=20)\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# Perform one hot encoding on label\n",
    "def one_hot_encode(label, label_values):\n",
    "    \"\"\"\n",
    "    Convert a segmentation image label array to one-hot format\n",
    "    by replacing each pixel value with a vector of length num_classes\n",
    "    # Arguments\n",
    "        label: The 2D array segmentation image label\n",
    "        label_values\n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of num_classes\n",
    "    \"\"\"\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "\n",
    "    return semantic_map\n",
    "    \n",
    "# Perform reverse one-hot-encoding on labels / preds\n",
    "def reverse_one_hot(image):\n",
    "    \"\"\"\n",
    "    Transform a 2D array in one-hot format (depth is num_classes),\n",
    "    to a 2D array with only 1 channel, where each pixel value is\n",
    "    the classified class key.\n",
    "    # Arguments\n",
    "        image: The one-hot format image \n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of 1, where each pixel value is the classified \n",
    "        class key.\n",
    "    \"\"\"\n",
    "    x = np.argmax(image, axis = -1)\n",
    "    return x\n",
    "\n",
    "# Perform colour coding on the reverse-one-hot outputs\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    \"\"\"\n",
    "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
    "    # Arguments\n",
    "        image: single channel array where each value represents the class key.\n",
    "        label_values\n",
    "\n",
    "    # Returns\n",
    "        Colour coded image for segmentation visualization\n",
    "    \"\"\"\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "\n",
    "    return x\n",
    "\n",
    "class A2D2_Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    \"\"\"Audi Autonomous Driving Dataset. \n",
    "       Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_list, \n",
    "            masks_list, \n",
    "            class_rgb_values=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        \n",
    "        self.image_paths = images_list\n",
    "        self.mask_paths = masks_list\n",
    "\n",
    "        self.class_rgb_values = class_rgb_values\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read images and masks\n",
    "        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # one-hot-encode the mask\n",
    "        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        # return length of \n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b4549f-4a44-4626-b9e6-a26e2058e55d",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "641b1d84-4987-45be-845f-07861fe06f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [    \n",
    "        album.RandomCrop(height=256, width=256, always_apply=True),\n",
    "        album.OneOf(\n",
    "            [\n",
    "                album.HorizontalFlip(p=1),\n",
    "                album.VerticalFlip(p=1),\n",
    "                album.RandomRotate90(p=1),\n",
    "            ],\n",
    "            p=0.75,\n",
    "        ),\n",
    "    ]\n",
    "    return album.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():   \n",
    "    # Add sufficient padding to ensure image is divisible by 32\n",
    "    test_transform = [\n",
    "        album.PadIfNeeded(min_height=1536, min_width=1536, always_apply=True, border_mode=0),\n",
    "    ]\n",
    "    return album.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn=None):\n",
    "    \"\"\"Construct preprocessing transform    \n",
    "    Args:\n",
    "        preprocessing_fn (callable): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \"\"\"   \n",
    "    _transform = []\n",
    "    if preprocessing_fn:\n",
    "        _transform.append(album.Lambda(image=preprocessing_fn))\n",
    "    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n",
    "        \n",
    "    return album.Compose(_transform)\n",
    "\n",
    "ENCODER = 'resnet50'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = class_names\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4399ce96-a3ba-4afd-8afe-e8e388d6f6c4",
   "metadata": {},
   "source": [
    "### Define training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bf58985-5e65-42f8-be77-37f2fb238f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and val dataset instances\n",
    "train_dataset = A2D2_Dataset(\n",
    "    x_train_dir, y_train_dir, \n",
    "    # augmentation=None,\n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "valid_dataset = A2D2_Dataset(\n",
    "    val_images_paths, val_labels_paths, \n",
    "    # augmentation=None,\n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n",
    "TRAINING = True\n",
    "\n",
    "# Set device: `cuda` or `cpu`\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# define loss function\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "\n",
    "# define metrics\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.0001),\n",
    "])\n",
    "\n",
    "# define learning rate scheduler (not used in this NB)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n",
    ")\n",
    "\n",
    "# load best saved model checkpoint from previous commit (if present)\n",
    "if os.path.exists('best_resnet50_model.pth'):\n",
    "    model = torch.load('best_resnet50_model.pth', map_location=DEVICE)\n",
    "\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Set num of epochs\n",
    "EPOCHS = 2\n",
    "\n",
    "# Get train and val data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249be601-5679-4cb2-b358-80fcd0064173",
   "metadata": {},
   "source": [
    "### Run training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14d492b1-c3d7-4c84-a961-39cd011cb1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "train: 100%|██████████| 1359/1359 [3:22:38<00:00,  8.95s/it, dice_loss - 0.6043, iou_score - 0.302]  \n",
      "valid: 100%|██████████| 283/283 [44:44<00:00,  9.49s/it, dice_loss - 0.3245, iou_score - 0.5535]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 1\n",
      "train: 100%|██████████| 1359/1359 [3:26:36<00:00,  9.12s/it, dice_loss - 0.4699, iou_score - 0.4093] \n",
      "valid: 100%|██████████| 283/283 [41:26<00:00,  8.79s/it, dice_loss - 0.2486, iou_score - 0.6474]\n",
      "Model saved!\n",
      "CPU times: user 8h 29min 47s, sys: 2h 5min, total: 10h 34min 47s\n",
      "Wall time: 8h 15min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if TRAINING:\n",
    "\n",
    "    best_iou_score = 0.0\n",
    "    train_logs_list, valid_logs_list = [], []\n",
    "\n",
    "    for i in range(0, EPOCHS):\n",
    "\n",
    "        # Perform training & validation\n",
    "        print('\\nEpoch: {}'.format(i))\n",
    "        train_logs = train_epoch.run(train_loader)\n",
    "        valid_logs = valid_epoch.run(valid_loader)\n",
    "        train_logs_list.append(train_logs)\n",
    "        valid_logs_list.append(valid_logs)\n",
    "\n",
    "        # Save model if a better val IoU score is obtained\n",
    "        if best_iou_score < valid_logs['iou_score']:\n",
    "            best_iou_score = valid_logs['iou_score']\n",
    "            torch.save(model, './best_resnet50_model.pth')\n",
    "            print('Model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7ba2771-967c-4480-9468-b9581a51e664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "score_train = [i[\"iou_score\"] for i in train_logs_list]\n",
    "score_valid = [i[\"iou_score\"] for i in valid_logs_list]\n",
    "\n",
    "ax.plot(score_train, label=\"train IoU\")\n",
    "ax.plot(score_valid, label=\"validation IoU\")\n",
    "ax.set_xticks([i for i in range(5)])\n",
    "\n",
    "plt.suptitle(\"IoU during train\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"IoU\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3195ba04-6f30-436c-9e3d-ac0a1aeb0d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of prediction: 33.47 ms\n"
     ]
    }
   ],
   "source": [
    "image, gt_mask = train_dataset[0]\n",
    "x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start.record()\n",
    "pred_mask = model(x_tensor)\n",
    "end.record()\n",
    "\n",
    "# Waits for everything to finish running\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(f\"Time of prediction: {start.elapsed_time(end):.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d319f-b78a-49ee-a56f-b59656d84a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2\n",
      "train: 100%|██████████| 1359/1359 [3:25:52<00:00,  9.09s/it, dice_loss - 0.3971, iou_score - 0.4834] \n",
      "valid: 100%|██████████| 283/283 [44:05<00:00,  9.35s/it, dice_loss - 0.2257, iou_score - 0.6864]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 3\n",
      "train: 100%|██████████| 1359/1359 [3:21:30<00:00,  8.90s/it, dice_loss - 0.3719, iou_score - 0.5107] \n",
      "valid: 100%|██████████| 283/283 [42:29<00:00,  9.01s/it, dice_loss - 0.1712, iou_score - 0.7614]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 4\n",
      "train:  74%|███████▍  | 1004/1359 [2:29:01<52:20,  8.85s/it, dice_loss - 0.3473, iou_score - 0.5345] "
     ]
    }
   ],
   "source": [
    "for i in range(2, 15):\n",
    "\n",
    "    # Perform training & validation\n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    train_logs_list.append(train_logs)\n",
    "    valid_logs_list.append(valid_logs)\n",
    "\n",
    "    # Save model if a better val IoU score is obtained\n",
    "    if best_iou_score < valid_logs['iou_score']:\n",
    "        best_iou_score = valid_logs['iou_score']\n",
    "        torch.save(model, f'best_{ENCODER}_model.pth')\n",
    "        print('Model saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
