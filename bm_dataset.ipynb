{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808e9b3f-f8ac-45c8-b256-71bb64b0f67b",
   "metadata": {},
   "source": [
    "# Benchmark creation. Dataset split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fda3009-9b48-4af2-aa7e-efebc9b879ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2022-06-19 10:40:17,452 - utils - Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO - 2022-06-19 10:40:17,453 - utils - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "# Import libs\n",
    "from global_names import A2D2_PATH\n",
    "import json\n",
    "import numpy as np\n",
    "from tutorial_modules import *\n",
    "from os.path import sep as os_sep\n",
    "from os.path import join as join_path\n",
    "from os.path import exists as path_exists\n",
    "from glob import glob as gg\n",
    "from random import choice as r_ch\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Load files\n",
    "with open(join_path(A2D2_PATH, \"cams_lidars.json\"), \"rb\") as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "# Load files\n",
    "with open(join_path(A2D2_PATH, \"camera_lidar_semantic\", \"class_list.json\"), \"rb\") as f:\n",
    "     class_list= json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ba0e85-907f-471f-94e2-e275c11e74ef",
   "metadata": {},
   "source": [
    "В датасете присутствуют данные для семантической сегментации, при этом не для каждого изображения есть разбивки и данные дальномера.\n",
    "В будущем для сравнения эффективности разных подходов будут использоваться в том числе техники слияния данных с разных сенсоров, поэтому для нашего \"бенчмарка\" нужно подобрать те наблюдения, которые представлены с разных сенсоров и для которых есть маска сегментации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2467813-43ff-48d0-887a-667489d99412",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_p = join_path(A2D2_PATH, \"camera_lidar_semantic\")\n",
    "parent_folders, sensor_types, sensor_aligns, all_files = [np.unique([p.split(\"/\")[5:][i] for p in gg(join_path(ss_p, \"*/*/*/*\"))]) for i in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eef9ee-c682-4802-b24e-d34435c3b994",
   "metadata": {},
   "source": [
    "Для однозначной идентификации наблюдения достаточно указать через \"_\" дату, положение камеры и timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9310a582-318c-4e9b-a2db-551d549ab21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = sorted(set([ # remove duplicates and sort\n",
    "    \"_\".join([a,d,c]) for a,b,c,d in [ # create id\n",
    "        x.split(os_sep)[-1].split(\".\")[0].split(\"_\") for x in # split filename into parts\n",
    "        gg(join_path(ss_p, f\"*{os_sep}*{os_sep}*{os_sep}*\")) # find all data from all sensors\n",
    "    ]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b36cea12-50c3-4af7-bcc5-a7b068734607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sens_ext = {\n",
    "    \"camera\": \".png\",\n",
    "    \"label\": \".png\",\n",
    "    \"lidar\": \".npz\"\n",
    "}\n",
    "\n",
    "rel_ = lambda __p: join_path(*__p.split(os_sep)[__p.split(os_sep).index('camera_lidar_semantic'):])\n",
    "abs_ = lambda __p: join_path(A2D2_PATH, __p)\n",
    "\n",
    "def sensor_p(_id, s_type):\n",
    "    if s_type not in sens_ext.keys(): raise ValueError(\"Wrong sensor type: s_type\")\n",
    "    d,t,s = _id.split(\"_\")\n",
    "    _p = \"_\".join([d, s_type, s, t]) + sens_ext[s_type]\n",
    "    _p = join_path(ss_p, f\"{d[:8]}_{d[8:]}\", s_type, f\"cam_{sa_(s)}\", _p)\n",
    "    return _p\n",
    "\n",
    "def sa_(x):\n",
    "    als = [\"center\", \"left\", \"right\"]\n",
    "    for o in als:\n",
    "        if o in x:\n",
    "            return x.replace(o, \"_\" + o)\n",
    "    raise ValueError(f\"Bad index contains wrong sensor align: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a00e3ba2-5bfb-4123-99d8-b876a51bca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ids = []\n",
    "for _id in ids:\n",
    "    _exists = True\n",
    "    for i in sensor_types:\n",
    "        _exists = _exists and path_exists(sensor_p(_id, i))\n",
    "    if _exists:\n",
    "        ds_ids.append(_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b104173-78ff-4545-bfd8-c2b2058c4946",
   "metadata": {},
   "source": [
    "При разбиении датасета на выборки, важно учитывать временную составляющую, поскольку многие потенциально эффективные нейронные сети используют рекуррентные слои. Поэтому важно отсортировать id по дате и дню, а также не разбрасывать данные одного наблюдения разных сенсоров по выборкам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8018a487-bd79-4b1f-8afb-d726385afce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5655/5655 [00:18<00:00, 313.19it/s]\n",
      "100%|██████████| 5655/5655 [00:17<00:00, 315.38it/s]\n",
      "100%|██████████| 26388/26388 [01:24<00:00, 313.65it/s]\n"
     ]
    }
   ],
   "source": [
    "if not path_exists(\"bm_ds.pkl\"):\n",
    "    ds_pool = sorted(set([\"_\".join(i.split(\"_\")[:-1]) for i in ds_ids]))\n",
    "    val_size = test_size = round(len(ds_pool) * 0.15)\n",
    "    subsets = test_ids, val_ids, train_ids = [], [], []\n",
    "    namesplits = test_list, val_list, train_list = ds_pool[-test_size:], ds_pool[-test_size-val_size:-test_size], ds_pool[:-test_size-val_size]\n",
    "    for ss, ns in zip(subsets, namesplits):\n",
    "        for n in tqdm(ns):\n",
    "            for p in ds_ids:\n",
    "                if n in p:\n",
    "                    ss.append(p)\n",
    "    bm_ds = {\"test_ids\" : test_ids, \"val_ids\" : val_ids, \"train_ids\" : train_ids}\n",
    "    with open(\"bm_ds.pkl\", \"wb\") as f:\n",
    "        pickle.dump(bm_ds, f)\n",
    "else:\n",
    "    with open(\"bm_ds.pkl\", \"rb\") as f:\n",
    "        bm_ds = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
