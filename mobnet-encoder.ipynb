{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e7ea8a2-0d10-4b8b-a501-0be37ef8d2d5",
   "metadata": {},
   "source": [
    "### Import libs, global variables and pickled files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba04c81c-14ce-48f8-9385-b1de128cf4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os, cv2, random, tqdm, json\n",
    "from random import choice as r_ch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from os.path import join as join_path\n",
    "from os.path import sep as os_sep\n",
    "from os.path import exists as path_exists\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as album\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from global_names import A2D2_PATH\n",
    "\n",
    "\n",
    "with open(\"bm_ds.pkl\", \"rb\") as f:\n",
    "    bm_ds = pickle.load(f)\n",
    "    \n",
    "# Load files\n",
    "with open(join_path(A2D2_PATH, \"camera_lidar_semantic\", \"class_list.json\"), \"rb\") as f:\n",
    "     class_list= json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9a7a8c-1d12-4e88-a431-471a9a3716f3",
   "metadata": {},
   "source": [
    "### Define functions for getting paths from ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d32b6aad-6fb9-47c0-ae25-d5bc662cd26d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ss_p = join_path(A2D2_PATH, \"camera_lidar_semantic\")\n",
    "\n",
    "sens_ext = {\n",
    "    \"camera\": \".png\",\n",
    "    \"label\": \".png\",\n",
    "    \"lidar\": \".npz\"\n",
    "}\n",
    "\n",
    "rel_ = lambda __p: join_path(*__p.split(os_sep)[__p.split(os_sep).index('camera_lidar_semantic'):])\n",
    "abs_ = lambda __p: join_path(A2D2_PATH, __p)\n",
    "\n",
    "def sensor_p(_id, s_type):\n",
    "    if s_type not in sens_ext.keys(): raise ValueError(\"Wrong sensor type: s_type\")\n",
    "    d,t,s = _id.split(\"_\")\n",
    "    _p = \"_\".join([d, s_type, s, t]) + sens_ext[s_type]\n",
    "    _p = join_path(ss_p, f\"{d[:8]}_{d[8:]}\", s_type, f\"cam_{sa_(s)}\", _p)\n",
    "    return rel_(_p)\n",
    "\n",
    "def sa_(x):\n",
    "    als = [\"center\", \"left\", \"right\"]\n",
    "    for o in als:\n",
    "        if o in x:\n",
    "            return x.replace(o, \"_\" + o)\n",
    "    raise ValueError(f\"Bad index contains wrong sensor align: {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8b5f8-92ae-47e4-be33-6d7bf775369f",
   "metadata": {},
   "source": [
    "### Create dataset from sparced ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff73284-5c82-44a2-aace-99de44e9aa47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It's important to reduce number of images\n",
    "sparced_ids = [val for i, val in enumerate(bm_ds[\"train_ids\"]) if i % 10 == 0]\n",
    "x_train_dir = np.array([abs_(sensor_p(p, \"camera\")) for p in sparced_ids])\n",
    "y_train_dir = np.array([abs_(sensor_p(p, \"label\")) for p in sparced_ids])\n",
    "\n",
    "# It's important to reduce number of images\n",
    "sparced_val_ids = [val for i, val in enumerate(bm_ds[\"val_ids\"]) if i % 10 == 0]\n",
    "val_images_paths = np.array([abs_(sensor_p(p, \"camera\")) for p in sparced_val_ids])\n",
    "val_labels_paths = np.array([abs_(sensor_p(p, \"label\")) for p in sparced_val_ids])\n",
    "\n",
    "class_names = list(class_list.values())\n",
    "class_rgb_values = [[int(i[1:3], 16), int(i[3:5], 16), int(i[5:7], 16)] for i in class_list.keys()]\n",
    "\n",
    "# Useful to shortlist specific classes in datasets with large number of classes\n",
    "select_classes = class_names # all classes\n",
    "\n",
    "# Get RGB values of required classes\n",
    "select_class_indices = [class_names.index(cls) for cls in select_classes]\n",
    "select_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n",
    "\n",
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"\n",
    "    Plot images in one row\n",
    "    \"\"\"\n",
    "    n_images = len(images)\n",
    "    plt.figure(figsize=(20,8))\n",
    "    for idx, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n_images, idx + 1)\n",
    "        plt.xticks([]); \n",
    "        plt.yticks([])\n",
    "        # get title from the parameter names\n",
    "        plt.title(name.replace('_',' ').title(), fontsize=20)\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# Perform one hot encoding on label\n",
    "def one_hot_encode(label, label_values):\n",
    "    \"\"\"\n",
    "    Convert a segmentation image label array to one-hot format\n",
    "    by replacing each pixel value with a vector of length num_classes\n",
    "    # Arguments\n",
    "        label: The 2D array segmentation image label\n",
    "        label_values\n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of num_classes\n",
    "    \"\"\"\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "\n",
    "    return semantic_map\n",
    "    \n",
    "# Perform reverse one-hot-encoding on labels / preds\n",
    "def reverse_one_hot(image):\n",
    "    \"\"\"\n",
    "    Transform a 2D array in one-hot format (depth is num_classes),\n",
    "    to a 2D array with only 1 channel, where each pixel value is\n",
    "    the classified class key.\n",
    "    # Arguments\n",
    "        image: The one-hot format image \n",
    "        \n",
    "    # Returns\n",
    "        A 2D array with the same width and hieght as the input, but\n",
    "        with a depth size of 1, where each pixel value is the classified \n",
    "        class key.\n",
    "    \"\"\"\n",
    "    x = np.argmax(image, axis = -1)\n",
    "    return x\n",
    "\n",
    "# Perform colour coding on the reverse-one-hot outputs\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    \"\"\"\n",
    "    Given a 1-channel array of class keys, colour code the segmentation results.\n",
    "    # Arguments\n",
    "        image: single channel array where each value represents the class key.\n",
    "        label_values\n",
    "\n",
    "    # Returns\n",
    "        Colour coded image for segmentation visualization\n",
    "    \"\"\"\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "\n",
    "    return x\n",
    "\n",
    "class A2D2_Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    \"\"\"Audi Autonomous Driving Dataset. \n",
    "       Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_list, \n",
    "            masks_list, \n",
    "            class_rgb_values=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        \n",
    "        self.image_paths = images_list\n",
    "        self.mask_paths = masks_list\n",
    "\n",
    "        self.class_rgb_values = class_rgb_values\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read images and masks\n",
    "        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # one-hot-encode the mask\n",
    "        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        # return length of \n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b4549f-4a44-4626-b9e6-a26e2058e55d",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fade33a-c9e2-442b-b5ee-d3f619baeb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [    \n",
    "        album.RandomCrop(height=256, width=256, always_apply=True),\n",
    "        album.OneOf(\n",
    "            [\n",
    "                album.HorizontalFlip(p=1),\n",
    "                album.VerticalFlip(p=1),\n",
    "                album.RandomRotate90(p=1),\n",
    "            ],\n",
    "            p=0.75,\n",
    "        ),\n",
    "    ]\n",
    "    return album.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():   \n",
    "    # Add sufficient padding to ensure image is divisible by 32\n",
    "    test_transform = [\n",
    "        album.PadIfNeeded(min_height=1536, min_width=1536, always_apply=True, border_mode=0),\n",
    "    ]\n",
    "    return album.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn=None):\n",
    "    \"\"\"Construct preprocessing transform    \n",
    "    Args:\n",
    "        preprocessing_fn (callable): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \"\"\"   \n",
    "    _transform = []\n",
    "    if preprocessing_fn:\n",
    "        _transform.append(album.Lambda(image=preprocessing_fn))\n",
    "    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n",
    "        \n",
    "    return album.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a224dfd5-8d09-43c5-a296-ca239080261f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /home/g.leontiev/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7da8fc5a6b4cab87f4208362cd3eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ENCODER = 'mobilenet_v2'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = class_names\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.DeepLabV3Plus(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d1f6759-cbcb-4359-8240-ea1ab3c9ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_en = ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext101_32x4d', 'resnext101_32x8d', 'resnext101_32x16d', 'resnext101_32x32d', 'resnext101_32x48d', 'dpn68', 'dpn68b', 'dpn92', 'dpn98', 'dpn107', 'dpn131', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152', 'se_resnext50_32x4d', 'se_resnext101_32x4d', 'densenet121', 'densenet169', 'densenet201', 'densenet161', 'inceptionresnetv2', 'inceptionv4', 'efficientnet-b0', 'efficientnet-b1', 'efficientnet-b2', 'efficientnet-b3', 'efficientnet-b4', 'efficientnet-b5', 'efficientnet-b6', 'efficientnet-b7', 'mobilenet_v2', 'xception', 'timm-efficientnet-b0', 'timm-efficientnet-b1', 'timm-efficientnet-b2', 'timm-efficientnet-b3', 'timm-efficientnet-b4', 'timm-efficientnet-b5', 'timm-efficientnet-b6', 'timm-efficientnet-b7', 'timm-efficientnet-b8', 'timm-efficientnet-l2', 'timm-tf_efficientnet_lite0', 'timm-tf_efficientnet_lite1', 'timm-tf_efficientnet_lite2', 'timm-tf_efficientnet_lite3', 'timm-tf_efficientnet_lite4', 'timm-resnest14d', 'timm-resnest26d', 'timm-resnest50d', 'timm-resnest101e', 'timm-resnest200e', 'timm-resnest269e', 'timm-resnest50d_4s2x40d', 'timm-resnest50d_1s4x24d', 'timm-res2net50_26w_4s', 'timm-res2net101_26w_4s', 'timm-res2net50_26w_6s', 'timm-res2net50_26w_8s', 'timm-res2net50_48w_2s', 'timm-res2net50_14w_8s', 'timm-res2next50', 'timm-regnetx_002', 'timm-regnetx_004', 'timm-regnetx_006', 'timm-regnetx_008', 'timm-regnetx_016', 'timm-regnetx_032', 'timm-regnetx_040', 'timm-regnetx_064', 'timm-regnetx_080', 'timm-regnetx_120', 'timm-regnetx_160', 'timm-regnetx_320', 'timm-regnety_002', 'timm-regnety_004', 'timm-regnety_006', 'timm-regnety_008', 'timm-regnety_016', 'timm-regnety_032', 'timm-regnety_040', 'timm-regnety_064', 'timm-regnety_080', 'timm-regnety_120', 'timm-regnety_160', 'timm-regnety_320', 'timm-skresnet18', 'timm-skresnet34', 'timm-skresnext50_32x4d', 'timm-mobilenetv3_large_075', 'timm-mobilenetv3_large_100', 'timm-mobilenetv3_large_minimal_100', 'timm-mobilenetv3_small_075', 'timm-mobilenetv3_small_100', 'timm-mobilenetv3_small_minimal_100', 'timm-gernet_s', 'timm-gernet_m', 'timm-gernet_l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c47b40b-5f32-43c5-979f-79b18ebfb8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mobilenet_v2',\n",
       " 'timm-mobilenetv3_large_075',\n",
       " 'timm-mobilenetv3_large_100',\n",
       " 'timm-mobilenetv3_large_minimal_100',\n",
       " 'timm-mobilenetv3_small_075',\n",
       " 'timm-mobilenetv3_small_100',\n",
       " 'timm-mobilenetv3_small_minimal_100']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in list_en if \"mob\" in i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4399ce96-a3ba-4afd-8afe-e8e388d6f6c4",
   "metadata": {},
   "source": [
    "### Define training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61203e4b-1a51-444c-ab1e-889b25437cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and val dataset instances\n",
    "train_dataset = A2D2_Dataset(\n",
    "    x_train_dir, y_train_dir, \n",
    "    # augmentation=None,\n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "valid_dataset = A2D2_Dataset(\n",
    "    val_images_paths, val_labels_paths, \n",
    "    # augmentation=None,\n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c068f880-a7c5-4462-9f70-92d7a216d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n",
    "TRAINING = True\n",
    "\n",
    "# Set device: `cuda` or `cpu`\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# define loss function\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "\n",
    "# define metrics\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.0001),\n",
    "])\n",
    "\n",
    "# define learning rate scheduler (not used in this NB)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "244291b8-ef12-4b79-bdb7-d4f0203896a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best saved model checkpoint from previous commit (if present)\n",
    "if os.path.exists(f'best_{ENCODER}_model.pth'):\n",
    "    model = torch.load(f'best_{ENCODER}_model.pth', map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ff2ca41-e58e-4476-9d06-d3c41adb2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0fab779-e25f-4913-9703-56299b1dd02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set num of epochs\n",
    "EPOCHS = 2\n",
    "\n",
    "# Get train and val data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249be601-5679-4cb2-b358-80fcd0064173",
   "metadata": {},
   "source": [
    "### Run training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14d492b1-c3d7-4c84-a961-39cd011cb1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "train: 100%|██████████| 1359/1359 [3:24:48<00:00,  9.04s/it, dice_loss - 0.6472, iou_score - 0.2647] \n",
      "valid: 100%|██████████| 283/283 [42:18<00:00,  8.97s/it, dice_loss - 0.2791, iou_score - 0.6212]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 1\n",
      "train: 100%|██████████| 1359/1359 [3:26:26<00:00,  9.11s/it, dice_loss - 0.491, iou_score - 0.3806]  \n",
      "valid: 100%|██████████| 283/283 [42:20<00:00,  8.98s/it, dice_loss - 0.2009, iou_score - 0.7125]\n",
      "Model saved!\n",
      "CPU times: user 8h 37min 5s, sys: 2h 3min 32s, total: 10h 40min 37s\n",
      "Wall time: 8h 15min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if TRAINING:\n",
    "\n",
    "    best_iou_score = 0.0\n",
    "    train_logs_list, valid_logs_list = [], []\n",
    "\n",
    "    for i in range(0, EPOCHS):\n",
    "\n",
    "        # Perform training & validation\n",
    "        print('\\nEpoch: {}'.format(i))\n",
    "        train_logs = train_epoch.run(train_loader)\n",
    "        valid_logs = valid_epoch.run(valid_loader)\n",
    "        train_logs_list.append(train_logs)\n",
    "        valid_logs_list.append(valid_logs)\n",
    "\n",
    "        # Save model if a better val IoU score is obtained\n",
    "        if best_iou_score < valid_logs['iou_score']:\n",
    "            best_iou_score = valid_logs['iou_score']\n",
    "            torch.save(model, f'best_{ENCODER}_model.pth')\n",
    "            print('Model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7ba2771-967c-4480-9468-b9581a51e664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "score_train = [i[\"iou_score\"] for i in train_logs_list]\n",
    "score_valid = [i[\"iou_score\"] for i in valid_logs_list]\n",
    "\n",
    "ax.plot(score_train, label=\"train IoU\")\n",
    "ax.plot(score_valid, label=\"validation IoU\")\n",
    "ax.set_xticks([i for i in range(5)])\n",
    "\n",
    "plt.suptitle(\"IoU during train\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"IoU\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3066f3-17b2-4801-9797-8d41e4421be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of prediction: 30.48 ms\n"
     ]
    }
   ],
   "source": [
    "image, gt_mask = train_dataset[0]\n",
    "x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start.record()\n",
    "pred_mask = model(x_tensor)\n",
    "end.record()\n",
    "\n",
    "# Waits for everything to finish running\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(f\"Time of prediction: {start.elapsed_time(end):.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36db3944-5a6a-4ae7-8a1b-2716596fbd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2\n",
      "train: 100%|██████████| 1359/1359 [3:27:23<00:00,  9.16s/it, dice_loss - 0.4539, iou_score - 0.417]  \n",
      "valid: 100%|██████████| 283/283 [42:31<00:00,  9.01s/it, dice_loss - 0.1964, iou_score - 0.7106]\n",
      "\n",
      "Epoch: 3\n",
      "train: 100%|██████████| 1359/1359 [3:28:17<00:00,  9.20s/it, dice_loss - 0.4017, iou_score - 0.4748] \n",
      "valid: 100%|██████████| 283/283 [43:29<00:00,  9.22s/it, dice_loss - 0.1813, iou_score - 0.7449]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 4\n",
      "train:  68%|██████▊   | 923/1359 [2:21:43<1:05:14,  8.98s/it, dice_loss - 0.3817, iou_score - 0.4944]"
     ]
    }
   ],
   "source": [
    "for i in range(2, 15):\n",
    "\n",
    "    # Perform training & validation\n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    train_logs_list.append(train_logs)\n",
    "    valid_logs_list.append(valid_logs)\n",
    "\n",
    "    # Save model if a better val IoU score is obtained\n",
    "    if best_iou_score < valid_logs['iou_score']:\n",
    "        best_iou_score = valid_logs['iou_score']\n",
    "        torch.save(model, f'best_{ENCODER}_model.pth')\n",
    "        print('Model saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
